{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = CSVLoader(file_path=\"C:/Users/Deepin/Desktop/shivam/RAG Projects/RAG Projects/data/test.csv\")\n",
    "# loader = TextLoader(file_path=\"C:/Users/Deepin/Desktop/shivam/RAG Projects/RAG Projects/data/test.txt\")\n",
    "# loader = PyMuPDFLoader(file_path=\"C:/Users/Deepin/Desktop/shivam/RAG Projects/RAG Projects/data/test.pdf\")\n",
    "# loader = Docx2txtLoader(\"data/test.docx\")\n",
    "# loader = UnstructuredURLLoader(urls = ['https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf'])\n",
    "# loader = WebBaseLoader(\"https://medium.com/@varsha.rainer/document-loaders-in-langchain-7c2db9851123\")\n",
    "# loader.load()\n",
    "# pagee = pages[0]\n",
    "# print(pagee.page_content[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basic chat bot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohere embeddings loaded\n",
      "Response: {'question': 'hi', 'chat_history': [HumanMessage(content='hi'), AIMessage(content=\"I don't know.\")], 'answer': \"I don't know.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings, CohereEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import streamlit as st\n",
    "import os\n",
    "import glob\n",
    "from typing import Union\n",
    "from io import BytesIO\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from multiprocessing import Pool\n",
    "from constants import CHROMA_SETTINGS\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import time\n",
    "from PIL import Image\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS,Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.document_loaders import TextLoader, PyMuPDFLoader, Docx2txtLoader, UnstructuredURLLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "def get_text_chunks(results, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(results)\n",
    "    return texts \n",
    "\n",
    "def get_vectorstore(results, embeddings_model_name, persist_directory, client_settings, chunk_size, chunk_overlap):\n",
    "    if embeddings_model_name == \"openai\":     \n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        print('OpenAI embeddings loaded')\n",
    "    elif embeddings_model_name == \"Cohereembeddings\":\n",
    "        embeddings = CohereEmbeddings()\n",
    "        print('Cohere embeddings loaded')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported embeddings model name.\")\n",
    "       \n",
    "    texts = get_text_chunks(results, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "    db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=client_settings)\n",
    "    db.add_documents(texts)\n",
    "    return db\n",
    "\n",
    "def get_conversation_chain(vectorstore, target_source_chunks, model_type):\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": target_source_chunks})\n",
    "    \n",
    "    match model_type:\n",
    "        case \"OpenaAI\":\n",
    "            llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "        case \"Llama3\":\n",
    "            llm = Ollama(model=\"llama3\")\n",
    "        case _default:\n",
    "            raise Exception(f\"Model type {model_type} is not supported. Please choose one of the following: OpenaAI, Llama3\")\n",
    "    \n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory\n",
    "    )\n",
    "    return conversation_chain\n",
    "\n",
    "def load_input_file(input_file):\n",
    "    extension = input_file.split(\".\")[-1]\n",
    "    first_extension = input_file.split(\":\")[0]\n",
    "\n",
    "    if first_extension.lower() in [\"http\", \"https\"]:\n",
    "        if extension == \"pdf\":\n",
    "            loader = UnstructuredURLLoader(urls=[input_file])\n",
    "        else:\n",
    "            loader = WebBaseLoader(input_file)\n",
    "    else:\n",
    "        match extension:\n",
    "            case \"txt\":\n",
    "                loader = TextLoader(file_path=input_file)\n",
    "            case \"pdf\":\n",
    "                loader = PyMuPDFLoader(file_path=input_file)\n",
    "            case \"docx\":\n",
    "                loader = Docx2txtLoader(file_path=input_file)\n",
    "            case _:\n",
    "                raise ValueError(f\"Unsupported file extension: {extension}\")\n",
    "    \n",
    "    return loader.load()\n",
    "\n",
    "# Main flow\n",
    "input_file = input(\"Enter file name: \")\n",
    "raw_text = load_input_file(input_file)\n",
    "\n",
    "chunk_size = 1000  # Adjust as needed\n",
    "chunk_overlap = 200  # Adjust as needed\n",
    "embeddings_model_name = os.environ.get('EMBEDDINGS_MODEL_NAME')\n",
    "persist_directory = os.environ.get('PERSIST_DIRECTORY')\n",
    "client_settings = {}  # Replace with actual client settings if needed\n",
    "target_source_chunks = 5  # Adjust as needed\n",
    "model_type=os.environ.get('MODEL_TYPE')\n",
    "\n",
    "text_chunks = get_text_chunks(results=raw_text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "vectorstore = get_vectorstore(results=text_chunks, embeddings_model_name=embeddings_model_name, persist_directory=persist_directory, client_settings=client_settings, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "conversation_chain = get_conversation_chain(vectorstore=vectorstore, target_source_chunks=target_source_chunks, model_type=model_type)\n",
    "\n",
    "# Now you can use the conversation_chain to ask questions\n",
    "question = input(\"Enter your question: \")\n",
    "response = conversation_chain(question)\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Langchain Langsmith Langserve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, File, UploadFile, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "from langchain.embeddings import OpenAIEmbeddings, CohereEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.document_loaders import TextLoader, PyMuPDFLoader, Docx2txtLoader, UnstructuredURLLoader, WebBaseLoader\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Define constants\n",
    "CHROMA_SETTINGS = {}  # Replace with your actual settings if needed\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def get_text_chunks(results, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(results)\n",
    "    return texts \n",
    "\n",
    "def get_vectorstore(results, embeddings_model_name, persist_directory, client_settings, chunk_size, chunk_overlap):\n",
    "    if embeddings_model_name == \"openai\":     \n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        print('OpenAI embeddings loaded')\n",
    "    elif embeddings_model_name == \"Cohereembeddings\":\n",
    "        embeddings = CohereEmbeddings()\n",
    "        print('Cohere embeddings loaded')\n",
    "    else:\n",
    "        print(f\"Embeddings model name provided: {embeddings_model_name}\")  # Debug statement\n",
    "        raise ValueError(\"Unsupported embeddings model name.\")\n",
    "       \n",
    "    texts = get_text_chunks(results, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=client_settings)\n",
    "    db.add_documents(texts)\n",
    "    return db\n",
    "\n",
    "def get_conversation_chain(vectorstore, target_source_chunks, model_type):\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": target_source_chunks})\n",
    "    \n",
    "    if model_type == \"OpenAI\":\n",
    "        llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    elif model_type == \"Llama3\":\n",
    "        llm = Ollama(model=\"llama3\")\n",
    "    else:\n",
    "        raise Exception(f\"Model type {model_type} is not supported. Please choose one of the following: OpenAI, Llama3\")\n",
    "    \n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory\n",
    "    )\n",
    "    return conversation_chain\n",
    "\n",
    "def load_input_file(file_path):\n",
    "    extension = file_path.split(\".\")[-1]\n",
    "    first_extension = file_path.split(\":\")[0]\n",
    "\n",
    "    if first_extension.lower() in [\"http\", \"https\"]:\n",
    "        if extension == \"pdf\":\n",
    "            loader = UnstructuredURLLoader(urls=[file_path])\n",
    "        else:\n",
    "            loader = WebBaseLoader(file_path)\n",
    "    else:\n",
    "        if extension == \"txt\":\n",
    "            loader = TextLoader(file_path=file_path)\n",
    "        elif extension == \"pdf\":\n",
    "            loader = PyMuPDFLoader(file_path=file_path)\n",
    "        elif extension == \"docx\":\n",
    "            loader = Docx2txtLoader(file_path=file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {extension}\")\n",
    "    \n",
    "    return loader.load()\n",
    "\n",
    "@app.post(\"/process_file/\")\n",
    "async def process_file(file: UploadFile = File(...), model_type: str = Form(...)):\n",
    "    temp_dir = \"temp\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    file_location = f\"{temp_dir}/{file.filename}\"\n",
    "    \n",
    "    with open(file_location, \"wb+\") as file_object:\n",
    "        file_object.write(file.file.read())\n",
    "\n",
    "    raw_text = load_input_file(file_location)\n",
    "    chunk_size = 1000  # Adjust as needed\n",
    "    chunk_overlap = 200  # Adjust as needed\n",
    "    embeddings_model_name = os.getenv('EMBEDDINGS_MODEL_NAME')\n",
    "    persist_directory = os.getenv('PERSIST_DIRECTORY')\n",
    "    client_settings = CHROMA_SETTINGS  # Replace with actual client settings if needed\n",
    "    target_source_chunks = 5  # Adjust as needed\n",
    "\n",
    "    # Debug statements\n",
    "    print(f\"Embeddings model name: {embeddings_model_name}\")\n",
    "    print(f\"Persist directory: {persist_directory}\")\n",
    "    print(f\"Model type: {model_type}\")\n",
    "\n",
    "    text_chunks = get_text_chunks(results=raw_text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    vectorstore = get_vectorstore(results=text_chunks, embeddings_model_name=embeddings_model_name, persist_directory=persist_directory, client_settings=client_settings, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    global conversation_chain\n",
    "    conversation_chain = get_conversation_chain(vectorstore=vectorstore, target_source_chunks=target_source_chunks, model_type=model_type)\n",
    "\n",
    "    return JSONResponse(content={\"message\": \"File processed successfully\", \"model_type\": model_type})\n",
    "\n",
    "@app.post(\"/ask_question/\")\n",
    "async def ask_question(question: str = Form(...)):\n",
    "    global conversation_chain\n",
    "    response = conversation_chain(question)\n",
    "\n",
    "    # Debugging the response\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "    if isinstance(response, dict) and 'result' in response:\n",
    "        response_text = response['result']\n",
    "    else:\n",
    "        response_text = str(response)\n",
    "    \n",
    "    return JSONResponse(content={\"response\": response_text})\n",
    "\n",
    "# To run the FastAPI app:\n",
    "# uvicorn main:app --reload\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
